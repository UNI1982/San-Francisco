---
title: "Final San Francisco Crime"
author: "Pedro Cornejo"
date: "02/14/2020"
output:
  pdf_document: default
  html_document: default
  
fontsize: 12pt
geometry: margin = 1.0 in
---
---

# **Introduccion**.
The data provided about San Francisco city crime classification come from Kaggle's competition. The goal is to create a model able to predict crime in this city using the data between 2014 and 2015, which has 65534 observations and 9 features.--

During the process of designing the model, the maximum accuracy is achieved when data sets are filter over districts and high rated crimes. The performance is more accurate because the data is focalized in a specific area and crimes, which is known as bucketing data set to improve performance and accuracy in data set where classification is the goal.--

The hypothesis is that Crime is a function of time and its location. The data visualization and correlation analysis will verify that this hypothesis is supported, and then using the statistical models and tunning them look for the highest accuracy results.
```{r}
# Loading libraries needed in the analysis.

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")   
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(gplots)) install.packages("gplots", repos = "http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(here)) install.packages("here", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(lubridate)
library(kableExtra)
library(data.table)
library(e1071)
library(RColorBrewer)
library(gplots)
library(corrplot)
library(here)
```

## **Getting the data**. 
The data is dowloaded from: https://www.kaggle.com/c/sf-crime/download/RiTVAa9kf1hu9l7TmtUX%2Fversions%2FNRHocVFvjrC3Q7lrLMcf%2Ffiles%2Ftest.csv.zip, and 
https://www.kaggle.com/c/sf-crime/download/RiTVAa9kf1hu9l7TmtUX%2Fversions%2FNRHocVFvjrC3Q7lrLMcf%2Ffiles%2Ftrain.csv.zip. Kaggle website requires to login to load the data.--

 The train and test sets are zipped, and they should be loaded in a folder named "data", which is a subfolder where the R.project is located.  The function here() allows running the code in any OS and computer, openning the R.project file the file will pull the data in any system configuration without using path or system file functions. --
Barrett,2019 explained that using set wd, path, and rm ls functions lack accuracy and create extra issues. The author states "If the first line of your R script is: setwd("c:\..\.."). I will come into your office and SET YOUR COMPUTER ON FIRE. If the first line of your R script is: rm(list = ls()). I will come into your office and SET YOUR COMPUTER ON FIRE."--

The author explains that any project should load up on any computer, and it must "just work‚Äù. Therefore, the Here function will help to load projects in any operating system and computer, just saving the data folder where the R.project file is located. This can be proved using the github: https://github.com/UNI1982, where this file project is saved, and runs in any computer, even in a laptop from work. It just works.
---
```{r}
################################
# Data Sets acquisition
################################
 # San Francisco datasets:
# https://www.kaggle.com/c/sf-crime/download/RiTVAa9kf1hu9l7TmtUX%2Fversions%2FNRHocVFvjrC3Q7lrLMcf%2Ffiles%2Ftest.csv.zip
# https://www.kaggle.com/c/sf-crime/download/RiTVAa9kf1hu9l7TmtUX%2Fversions%2FNRHocVFvjrC3Q7lrLMcf%2Ffiles%2Ftrain.csv.zip
# kaggle requires to login to download the files
 


# Getting data load requires to use of a data folder located in
# the same main folder were Rproject is located
 
here()
getwd()
test <- read.csv(here("data","SanFrancisco_test.csv"), stringsAsFactors = FALSE)
train<- read.csv(here("data","SanFrancisco_train.csv"),stringsAsFactors = FALSE) 
str(test)
str(train)
dim(train)
dim(test) 
```

## **Train and Test Set up**.
 The str function shows two data sets with different dimensions. The test-set has only one possible factor to analyze, the PdDistrict, with the predictors' time and location features. On the other hand, the train set has the factors Category, the PdDistrict, which are the type of crimes and the district of occurrence, and the predictors' time and location respectively. The provided train-set gives a better opportunity to create a model that predicts the type of crime, the category column, related to PdDistrict, time, and location in San Francisco city.--

There are two solutions, one is to sample and split the train set to get the training and testing data for the model. The second option is to join the provided train and test datasets, and then, create the training, testing and validate data for the model will be created. The latter will be the approach in this analysis to have bigger data sets. To join the data sets, first, it is necessary to ensure that both train and test sets are not sharing data to avoid repetitive or missing data, and then create the training and testing sets for the project.

```{r}

# str and im functions help to know and check the data loaded to have 
# an idea # how to set it up for the future model.
# Spliting the train set into subsets for cross validation of the model
#  Checking if both datasets are unique
main_data <- full_join(train,test, by = NULL, type = "full", match = "all")
dim(main_data)
# Successfully jointed, the dimension of the main_data is the  sum of 
# dim(train) + dim(test) 
 
# Spliting the train set into subsets for cross validation of the model
# Set.seed(1) for R version before the 3.6 version
set.seed(1, sample.kind = "Rounding")
sampling_Trainset <- createDataPartition(y = main_data$Category, 
                                         p = 0.20, list = FALSE)

# creating the data sets shows TREA , which is denominated as Traspassing in the
# FBI crime definition. This is a unique data that will no affect our analysis,
#and it is included.  
training <- main_data[-sampling_Trainset, ]
testing <- main_data[sampling_Trainset,]
summary(training)

# The Id column has NAs that weres added in the Join process , we do not need them

training$Id <- NULL
testing$Id <- NULL

```

 
## **Cleaning up the data**.
The category column is a character variable that requires to be a factor to be part of the model. Model prediction predicts a factor depending on the numerical features, predictors. Therefore, the category column requires to be a factor, the predictors need to numerical.   

```{r}
# Category as a factor to be able to validate the model
 
training$Category <-  as.factor(training$Category)
# Factors to be predicted 
levels(training$Category)
# Checking any NA on data sets
anyNA(training) 

# There are 52427 NAs in Category, meaning there is no records on type of crime,
# so these data are not providing any information, so they can be eliminated

training <- training%>%filter(Category !="NA.")%>% droplevels()
levels(training$Category)
anyNA(training) 
summary(training)
# Now training is cleaned

# Cleaning testing data set
testing$Category <-  as.factor(testing$Category)
# Factors to be predicted 
levels(testing$Category)
# Checking any NA on data sets
anyNA(testing) 

# There are 52427 NAs in Category, meaning there is no records on type of crime,
# so these data are not providing any information, so they can be eliminated

testing <- testing%>%filter(Category !="NA.")%>% droplevels()
levels(testing$Category)
anyNA(testing) 
summary(testing)

# Data sets are cleaned and ready to be analysed
```
## ** Distribution of type Crime**.
On the graphic, the five more relevant of the 36 crimes are: **are Larceny, other offenses, non-criminal, assault, and vehicle theft**.  There are crimes in the very low ranges that perhaps are not influential in any district. The heatmaps will clarify if only high crimes on all districts have significant influence over the districts and locations.

```{r}
 
############ Data Visualization########################
 
 
Crimes_freq <- training %>% group_by(Category) %>% 
               summarise(Type_crimes = n()) %>% 
               arrange(desc(Type_crimes)) %>% ungroup()

# Using http://r-statistics.co/ggplot2-Tutorial-With-R.html as plotting guide:

Crimes_freq%>% ggplot(aes(reorder(Category,Type_crimes), y = Type_crimes, 
                          fill =Type_crimes)) + geom_col() + 
              coord_flip() +theme(legend.position = "top") + 
              labs( x = 'Type of Crimes', 
                    title = 'Frequency of Crimes in San Francisco')
nrow(Crimes_freq)

# 36 crimes that our graphic shows larceny, Assault , Vehiclue theft as the more 
# frequent type of crimea in all the districts.

```
 
## ** Setting up databases**.
Now, it is required to transform our predictors into numerical variables. The time and location are not simple variables. Time is a cyclical variable  that changes every 24hour, 12 months, and 365 days.--
The location is expressed on degrees because the data are expressed in spherical coordinates that it is used to calculate the longitude and latitude of any position on the planet. Therefore, both variables Location and time require a transformation to normalize and ensure the same scale.

**Location**.
R multipurpose kit,2015 gives the relationship between latitude and longitude to cartesian coordinates, which is the numerical distance at any point of the planet.
"x = radius * cos(latitude) * cos(longitude)
 y = radius * cos(latitude) * sin(longitude)
 z = radius * sin(latitude)"
The normalization of the varibles x,y,z, the vector(x,y,z),is dividing by the radius. The main goal of this tranformation is to normalized the data to ensure the same scale distance variables.
```{r}
#LOCATION:
# Longitud lattidue are in spherical coordinates, which have degrees as scale.
# Therefore, those variable need to be in cartesian coordenates to avoid degrees,
# and standarized the data sets.

training$Location_x <- cos(training$Y)*cos(training$X)
training$Location_y <- cos(training$Y)* sin(training$X)
training$Location_z <- sin(training$Y)

```

**Time**
 Abramson,2019 presents an easy way to understand cyclical variables. This cycle is a unitary circle from 0 to 360, the period depends on the variable to be analyzed. For instance, in one month the cycle is every 12 month and a new cycle happens.--
 
 Cycical concepts are important because the distance between Januaries of different years is zero, computers are not able to know the difference. Therefore, it is necessary to tell the computer that a new cycle happens. Abramson established that any point in the circle, which is normalized because it is a unitary circle, is represented by the sin and cos.--
 
Therefore, the W position in the circle is:(sin(2piW/f), cos(2piW/f)). The "f" it is the frequency or the new cycle. For a month, it will be sin(2piMonth/12) and its os(2pi*Month/12). This concept is applied for the day, month and year on the training and testing datasets.


```{r}
# TIME

# The dates as POSIXct and characters as factors.
training$Dates <- ymd_hms(training$Dates)
training$Years <- year(training$Dates)
training$Months <- month(training$Dates)
training$Days <- day(training$Dates)
training$Hours <- hour(training$Dates)
training$PdDistrict <- as.factor(training$PdDistrict)
training$DayOfWeek <- wday(training$Dates)

# Time data is cyclical, so it is more accurate to use radial time
# approach to set up time data
training$Years_sin <- sin(2*pi*training$Years/365)
training$Years_cos <- cos(2*pi*training$Years/365) 
# For cyclical months
training$Months_sin <- sin(2*pi*training$Months/12) 
training$Months_cos <- cos(2*pi*training$Months/12)

# For cyclical hours and days
training$Days_sin <-  sin(2*pi*training$Days/30) 
training$Days_cos <- cos(2*pi*training$Days/30)
training$Hours_sin <- sin(2*pi*training$Hours/24) 
training$Hours_cos <- cos(2*pi*training$Hours/24) 


# The test-set must have in same paramenters to have a correct validation.

testing$Dates <- ymd_hms(testing$Dates)
testing$Years <- year(testing$Dates)
testing$Months <- month(testing$Dates)
testing$Days <- day(testing$Dates)
testing$Hours <- hour(testing$Dates)
testing$PdDistrict <- as.factor(testing$PdDistrict)
testing$DayOfWeek <- wday(testing$Dates) 
testing$Years_sin <- sin(2*pi*testing$Years/365)
testing$Years_cos <- cos(2*pi*testing$Years/365) 
testing$Months_sin <- sin(2*pi*testing$Months/12) 
testing$Months_cos <- cos(2*pi*testing$Months/12)
testing$Days_sin <-  sin(2*pi*testing$Days/30) 
testing$Days_cos <- cos(2*pi*testing$Days/30)
testing$Hours_sin <- sin(2*pi*testing$Hours/24) 
testing$Hours_cos <- cos(2*pi*testing$Hours/24) 
testing$Location_x <- cos(testing$Y)*cos(testing$X)
testing$Location_y <- cos(testing$Y)* sin(testing$X)
testing$Location_z <- sin(testing$Y) 

 
# ckeking if both data sets have the same columns
 
ifelse(all(sort(names(training)) %in% sort(names(testing))),
       "Identical data sets", "No ready")
ifelse(all(sapply(training, class) %in% sapply(testing, class)),
       "Same Classes", "No ready")

# The datasets can be leaner. Address is not required because longitude 
# and latidue are provided.
# descript and resolution are not part of the hypothesis.
# Elimating data that were transformed
 
training[c("Descript","Resolution", "Address","DayOfWeek","X", "Y", 
           "Years","Days","Hours")] <- list(NULL) 
testing[c("Descript","Resolution", "Address","DayOfWeek","X", "Y",
          "Years","Days","Hours")] <- list(NULL) 

# Ensuring no NAs and normalization
summary(training)
summary(testing)
 
# The datasets are normalized and ready to be used because
# The minimum and maximum do not have a big gaps.

```

**Data Visualization to ensure normalization**

The plot and boxplot show the cyclical variable behavior and the numerical normalized data set, respectively. There are not outliers and the cyclical variables can tell the model when new data starts a new cycle.--

The plot() shows a circle for the month because this is a cyclical data. The boxplot shows the data range and behaviors of the data. The graphics ensure that our data sets are normalized and ready to be used.

```{r}
#### CHECKING NORMALIZATION ######

# Checking if the data for time is cyclical
plot.data <- training
plot(plot.data$Months_sin,training$Months_cos, main="Month_sin as Cyclical Data")
 
# Checking data normalization
boxplot(plot.data[ , c(4:6,8:15)], main="Distribution of Normalized Data")
 
# Graphics show a range of -1.0 to 1.0. Data is normalized and ready to be used.
```
### **Correlation of crimes per district**

First checking the graphic of correlation, it is noticeable that the color blue is all over the graphic, which is according to the range a high 0.73 value.
Therefore, the hypothesis of crimes related to the districts and time is realistically measurable. The correlation with the other variables is similar. 
It is not present in this paper to not exaggerate the length of the presentation.
```{r}
###### Creating Correlation and Heatmaps of Crimes per Dictrict #######

# Guide gplots heatmap.2() features:  page 26 and 31 of #https://cran.r-project.org/web/packages/gplots/gplots.pdf.
 
CategoryPdDistrict_data <- training %>% 
                           group_by(Category, PdDistrict)%>% 
                           summarise(District_crimes = n())
CaPD <- CategoryPdDistrict_data %>% 
        group_by_at(vars(-District_crimes)) %>% 
        mutate(row_id=1:n()) %>% ungroup() %>%  
        spread(key=PdDistrict, value=District_crimes) %>%
        select(-row_id) 
head(CaPD)
# There are NAs over the districts. These NAs is because is very low crime or 
# no data were collected. So, it necessary to give a zero value instead
CaPD[is.na(CaPD)] <- 0

# CaPD needs to be a data frame to avoid the warning Tibble depreceated
CaPD <- as.data.frame(CaPD)
# Preparing the matrix  
row.names(CaPD) <- CaPD$Category

Matrix1_CaPd <-data.matrix(CaPD[,-1])
Matrix_CaPd <- Matrix1_CaPd[,-1]
head(Matrix_CaPd)
# Checking the correlation between variables to ensure that
# the hypothesis is on the correct direction.
m_cor <- cor(Matrix_CaPd)
corrplot(m_cor, type = "upper",mar=c(0,0,1,0), 
         main="Correlation of Crimes per District")
# The minimum correlation is 0.73, which corroborate how  the Type 
# of crime depend on the district.

```

## ** Heatmaps Crime per district**.
The heatmap shows a prominent increase of Larceny/theft and Assault as the main crimes over the districts and time. The most dangerous is the Southern district and the least dangerous Richmond and Park districts, which shows a unique pink color overall. The heatmaps are telling us that the hypothesis is in the correct direction because there is a high dependency on crimes on the Districts. 
```{r}
# The heat will tell the district with high and low crime  
coul2 <- colorRampPalette(brewer.pal(8, "PiYG"))(25)
CrimeDistrict_heatmap3 <- heatmap.2(Matrix_CaPd,                     
                          Colv=FALSE,     
                          srtCol=45,
                          Rowv=FALSE,     
                          dendrogram="none",
                          density.info="histogram",    
                          trace="none",              
                          col = coul2,          
                          cexRow=0.85,cexCol=0.75,
                          main = " Crimes per District")

```
## ** Heatmaps Type of Crimes Monthly**.
This heatmap shows how crime increase in certain months. Some high violent crimes are high during the whole year, but at the end of the year crimes increase substantially. 

```{r}
##### ** Heatmaps Type of Crimes Monthly #################
# Now correlation of category with Time. Months was choosen for 
# simplicity, but it coul be Hours, Years,

CategoryTime_data <- training %>% group_by(Category, Months)%>%
                     summarise(Monthly_crimes = n())
 
CaMonths <- CategoryTime_data %>% group_by_at(vars(-Monthly_crimes)) %>% 
            mutate(row_id=1:n()) %>% ungroup() %>%  
            spread(key=Months, value=Monthly_crimes) %>%     
            select(-row_id) 
CaMonths[is.na(CaMonths)] <- 0
CaMonths <- as.data.frame(CaMonths) 
row.names(CaMonths) <- CaMonths$Category
Matrix1_CaMonths <- data.matrix(CaMonths)
Matrix_CaMonths <- Matrix1_CaMonths[,-1]
head(Matrix_CaMonths)

coul3 <- colorRampPalette(brewer.pal(8, "RdBu"))(25)
heatmap_CategoryPdDistrict <- heatmap.2(Matrix_CaMonths,   
                                        Colv=FALSE,      
                                        srtCol=45,
                                        Rowv=FALSE,      
                                        dendrogram="none",
                                        density.info="histogram",    
                                        trace="none",               
                                        col = coul3,            
                                        cexRow=0.85,cexCol=0.75,
                                        xlab = "Months",   
                                        main = " Monthly Crimes")

```
## ** Heatmaps " Monthly Crimes per District"**.
The crime changes during the year, and it is higher in certain districts. Putting together all the heatmaps the type of crime has a correlation on the districts and
time of execution. The heatmaps are showing that the hypothesis is supported by the data provided. Meaning, it is correct but only using the same scenario of predictors.
The crimes depend directly on the Districts, time, and location, and PdDistricts show a different relationship with the type of crime and the time of execution.
 
```{r}
##### Heatmaps " Monthly Crimes per District" #############
# Now correlation of PdDistrict with Time. The correlation of the district 
# with time and category with time allow to understand the intrisic
# correlation of category with time and districts.

PdDistricTime_data <- training %>% group_by(PdDistrict, Months) %>% 
                      summarise(Monthly_crimes = n())
 
PdMonths <- PdDistricTime_data %>% group_by_at(vars(-Monthly_crimes)) %>% 
            mutate(row_id=1:n()) %>% ungroup() %>%  
            spread(key=Months, value=Monthly_crimes) %>%     
            select(-row_id) 

PdMonths[is.na(PdMonths)] <- 0
PdMonths <- as.data.frame(PdMonths) 
row.names(PdMonths) <- PdMonths$PdDistrict
Matrix1_PdMonths <- data.matrix(PdMonths)
Matrix_PdMonths<- Matrix1_PdMonths[,-1]
head(Matrix_PdMonths)
coul4 <- colorRampPalette(brewer.pal(8, "Reds"))(25)
heatmap_CategoryPdDistrict <- heatmap.2(Matrix_PdMonths,                      
                                        Colv=FALSE,      
                                        Rowv=FALSE,      
                                        srtCol=45,
                                        dendrogram="none",
                                        density.info="histogram",    
                                        trace="none",                
                                        col = coul4,           
                                        cexRow=0.85,cexCol=0.75,
                                        xlab = "Months",
                                        main = "Monthly Crimes per District")

# The heatmaps are telling that the type ofcrimes depends directly on 
# the Districts and on the time of execution.
```

The correlation map and heatmaps demonstrate that it is better to segment the data by the type of crime over the districts to ensure high accuracy. Now, in order to ensure the correct segmentation, the FBI crime characterization codes are used to have a more realistic model. Using the FBI's UCR codes will reduce the concern of bucketing incorrectly. As Developersgoogle,2019 explains splitting data must be done with caution because some buckets could have many points, while others few or none. 

```{r }
# the data required to be segmented over the crime type. Therefore, the FBI crime
#  characterization will give the correct segemnation of crimes. 

# Grouping according with FBI codes: 
#  https://ucr.fbi.gov/nibrs/2011/resources/nibrs-offense-codes/view

FBI_groupA <- c("ARSON","ASSAULT", "BRIBERY",     "BURGLARY","FRAUD","DRUG.NARCOTIC","EMBEZZLEMENT",
                "EXTORTION","SECONDARY.CODES",
                "FRAUD","FORGERY.COUNTERFEITING","GAMBLING",
                "KIDNAPPING","LARCENY.THEFT","MISSING.PERSON",
                "PROSTITUTION","ROBBERY","VANDALISM",
                "SEX.OFFENSES.FORCIBLE","SEX.OFFENSES.NON.FORCIBLE",
                "STOLEN.PROPERTY", "VEHICLE.THEFT","WEAPON.LAWS")

FBI_groupB <- c("BAD.CHECKS","DISORDERLY.CONDUCT","DRIVING.UNDER.THE.INFLUENCE",
                "DRUNKENNESS","FAMILY.OFFENSES","LIQUOR.LAWS",
                "LOITERING","NON.CRIMINAL","OTHER.OFFENSES","RUNAWAY",
                "SUICIDE","SUSPICIOUS.OCC","TRESPASS","WARRANTS")

FBI_violent <- c("ASSAULT","DRUG.NARCOTIC","KIDNAPPING","ROBBERY","DRUG.NARCOTIC") 
FBI_property <- c(FBI_groupB,FBI_groupA[!FBI_groupA %in% 
                c("ASSAULT","DRUG.NARCOTIC", "KIDNAPPING",
                  "ROBBERY","DRUG.NARCOTIC")])

# Bucketing Category , crime types to be more specific in the prediction 
Crime_groupA <-   training %>% filter(Category %in% FBI_groupA) %>%
                  droplevels()
Crime_groupA_test <- testing %>% filter(Category %in% FBI_groupA) %>%
                    droplevels()

Crime_groupB <-   training %>% filter(Category %in% FBI_groupB) %>%
                  droplevels()
Crime_groupB_test <- testing %>% filter(Category %in% FBI_groupB) %>%
                  droplevels()

Crime_violent <-   training %>% filter(Category %in% FBI_violent) %>% 
                  droplevels()
Crime_violent_test <- testing %>% filter(Category %in% FBI_violent) %>%
                  droplevels()

Crime_property<-   training %>% filter(Category %in% FBI_property) %>%
                  droplevels()
Crime_property_test <- testing %>% filter(Category %in% FBI_property) %>%
                  droplevels()


```

# **Modeling**
The modeling is under Lda, SVM, and Random Forest models. The data is normalized over distances. Therefore, the models can be used because classification models as SVM require numerical normalized distances to ensure accuracy. Irizarry, R.2019, shows us that a model starts Y ~ sum(sum of predictors) and the train() will get the directly the accuracy of the model over the training model that it is used.--

**LDA**.--

Starting with the model Lda modeling, the accuracy of San Francisco city without data grouping or bucketing is very low, the data set is spread over a too big area. Then, one more Lda modeling is used over the group of violent crime in the entire San Francisco city. The accuracy improves radically.--
The last Lda model uses a more specific data set, the violent crimes per district. This model produces the highest accuracy and it will be improved with SVM and Random Forest tunning.

```{r warning=FALSE}

##################### Hypothesis #############################################



# Hypothesis:Crimes (Category) depends on the week + hour +month + 
#                                            year + location (X+Y)
# Let's start looking for the best model to use. LDA is apply to  the whole
# data set to demostrate that it is require to split the Category variable into
# small fractions

Hypothesis <- Category ~ Years_sin + Years_cos + Months_sin+ Months_cos + 
              Hours_sin+ Hours_cos+ 
              Days_sin+ Days_cos+ 
              Location_x+ Location_y+ Location_z

##################### LDA Modelling #############################################

######## Entire San Francisco City ##############

# First Using the entire datasets

model.lda.SF<- train(Hypothesis, method = "lda", data = training)

# Cross-validation 
predictionSF <- predict(model.lda.SF, newdata = testing)
prediction_ldaSF <- factor(predictionSF, levels = levels(testing$Category))

# Accuracy
Accuracy_ldaSF <- confusionMatrix(prediction_ldaSF, testing$Category)$
                  overall["Accuracy"]

# Creating the table that will store all the Accuracies results to compare results
Accuracy_results.SF <- data_frame(method = " LDA on San Francisco City", 
                                Accuracy = Accuracy_ldaSF)

Accuracy_results.SF
# accuracy is 14% 


######## Using Violent data #################

# LDA over Violent crimes , which is the smallest group of crimes

model.lda.Violent <- train(Hypothesis, method = "lda", data = Crime_violent)

# Cross-validation 
predictionViolent <- predict(model.lda.Violent , newdata = Crime_violent_test)
prediction_ldaViolent <- factor(predictionViolent, 
                                levels = levels(Crime_violent_test$Category))

# Accuracy
Accuracy_ldaViolent <- confusionMatrix(prediction_ldaViolent, 
                                Crime_violent_test$Category)$overall["Accuracy"]

Accuracy_results.SF <- bind_rows(Accuracy_results.SF,
                          data_frame(method=" LDA on Violent Crimes in SF city",
                          Accuracy = Accuracy_ldaViolent ))

Accuracy_results.SF

######### Using High crimes rates on  San Francisco's districts######### 

# The highest accurary is predicting crimes over specific Districts 

Crimes_district <-  Crime_violent%>% filter(PdDistrict %in% 
                                    c("BAYVIEW","SOUTHERN"))%>%droplevels()

Crimes_district_test <- Crime_violent_test %>%
                        filter(PdDistrict %in% 
                        c("BAYVIEW","SOUTHERN"))%>%droplevels()

model.lda.District <- train(Hypothesis, method = "lda", 
                            data = Crimes_district)

# Cross-validation 
predictionDistrict<- predict(model.lda.District, 
                             newdata = Crimes_district_test)
prediction_ldaDistrict <- factor(predictionDistrict, 
                          levels = levels(Crimes_district_test$Category))

# Accuracy
Accuracy_ldaDistrict <- confusionMatrix(prediction_ldaDistrict, 
                          Crimes_district_test$Category)$overall["Accuracy"]
 
Accuracy_results.SF.Districts <- data_frame(method=
                                      "LDA on Violent Crime per District",
                                      Accuracy = Accuracy_ldaDistrict )
Accuracy_results.SF.Districts

```
## **SVM Modeling**

Shiyuan and Peng 2019 have a detail explanation of why the SVM is a Euclidean model. The predictors must be numerical and with the same dimensions because numerical distance are calculated as part of the statistical analysis. That is why, to include the categorical PdDistrict PdDistrict would require to transform it into numerical data, using dummy variables.  About the transformation of categorical data, Trochim,2006 explains that the best way is to use dummy variables function, which essentially gives a Boolean number to each categorical data to be transformed to numeric data. For instance A, B. C categorical data will be 001, 010, 011, this process will allow us to used categorical data without introducing new characteristics to the original data, and SVM will process the model without errors. This is  not part of this study so it is not included in the hypothesis.--
Predictors requirement of being numerical create the necessity of changing the location data from degrees dimensions to cartesian dimension, and the time dimension to a circular unitary data so the process of SVM will faster and increase accuracy.-- 
The SVM modeling is applied over the data sets with the highest accuracy found in the LDA models, the Violent crimes in San Francisco city and the data set on violent crimes per district. After tunning the SVM model the accuracy achieve is 77%  

```{r warning=FALSE}

##################### SVM Modeling ##############################################


# Svm method tuning Group A
grid <- expand.grid(C = c(0,0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
svm_model <- svm(Hypothesis, data = Crimes_district, method ="radial", 
                 trControl = trctrl, preProcess = c("center", "scale"),
                 tuneGrid = grid,tuneLength=10)
 
# better results gives cost= 1
svm_model <- svm(Hypothesis, data = Crimes_district, method ="radial",
                 trControl = trctrl, preProcess = c("center", "scale"),
                 cost= 1,tuneGrid = grid,tuneLength=10)

# Cross validation svm_model2
svm_prediction <- predict(svm_model, newdata = Crimes_district_test)
svm_prediction.District <- factor(svm_prediction, 
                              levels = levels(Crimes_district_test$Category))

Accuracy_svm <- confusionMatrix(data = svm_prediction.District, 
                                reference = Crimes_district_test$Category)$
                                overall["Accuracy"]

Accuracy_results.SF.Districts <- bind_rows(Accuracy_results.SF.Districts,
                              data_frame(method=
                              "SVM on Violent Crimes per District",
                              Accuracy = Accuracy_svm ))
Accuracy_results.SF.Districts
 

############### SVM Tunning #########################

District.crimes <-Crimes_district %>% 
                  filter(Category %in% 
                  c("ASSAULT","ROBBERY"))%>%droplevels() 

District.crimes.test <- Crimes_district_test %>% 
                        filter(Category %in% 
                        c("ASSAULT", "ROBBERY"))%>%droplevels()

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
svm_model.district.crimes <- svm(Hypothesis, data =District.crimes, 
                                 method ="linear", 
                                 trControl = trctrl, 
                                 preProcess = c("center", "scale"),
                                 tuneGrid = grid,tuneLength=10)

svm_district.crimes <- predict(svm_model.district.crimes,
                               newdata = District.crimes.test)
svm_district.crimes.type <- factor(svm_district.crimes, 
                            levels = levels(District.crimes.test$Category))

Accuracy_district_type <- confusionMatrix(data = svm_district.crimes.type, 
                              reference = District.crimes.test$Category)$
                              overall["Accuracy"] 
 
Accuracy_results.SF.Districts <- bind_rows(Accuracy_results.SF.Districts,
                              data_frame(method=
                              "SVM tunned on Violent Crimes per District",
                               Accuracy = Accuracy_district_type ))
Accuracy_results.SF.Districts


```
##** Random Forest**
The SVM model gave high accuracy results, but Random Forest could improve the results because it is a model that is accurate when classification is required. Predicting crimes  on districts as a function of time and location is a decision tree classification, and Random Forest perform was designed for that type of cases.--
Random Forest will be applied over the same data sets as the SVM modeling to compare results. The accuracy is 78% after tunning the model, giving the expected predictions .
```{r warning=FALSE}
##################### Random Forest Modelling #############################################

 

######### Using High crimes rates on  San Francisco's districts######### 

rf_training.districts <-train(Hypothesis,data =District.crimes,method="rf")
 
 
rf_district_type<- predict(rf_training.districts, 
                           newdata = District.crimes.test)
rf_district12_type <- factor(rf_district_type,
                        levels = levels(District.crimes.test$Category))

Accuracy_district_rf <- confusionMatrix(data = rf_district12_type, 
                                        reference = District.crimes.test$
                                          Category)$overall["Accuracy"] 
 
Accuracy_results.SF.Districts <- bind_rows(Accuracy_results.SF.Districts,
                              data_frame(method=
                                    "RF on Violent crimes per District",
                                    Accuracy = Accuracy_district_rf))
Accuracy_results.SF.Districts
 
############## Random Foresr Tunnig ###########################

trControl <- trainControl(method = "cv",
                          number = 10,
                          search = "grid")
## mtry###

set.seed(1234)
tuneGrid <- expand.grid(.mtry = c(5:15))
rf_mtry <- train(Hypothesis,
                 data = District.crimes,
                 method = "rf",
                 metric = "Accuracy",
                 tuneGrid = tuneGrid,
                 trControl = trControl,
                 nodesize = 14,
                 ntree = 300)
print(rf_mtry)
best_mtry <- rf_mtry$bestTune$mtry 
best_mtry

## Max nodes## 

store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = best_mtry)
trControl <- trainControl(method = "cv",
                          number = 10,
                          search = "grid")
for (maxnodes in c(30: 40)) {
  set.seed(1234)
  rf_maxnode <- train(Hypothesis,
                      data = District.crimes,
                      method = "rf",
                      metric = "Accuracy",
                      tuneGrid = tuneGrid,
                      trControl = trControl,
                      nodesize = 14,
                      maxnodes = maxnodes,
                      ntree = 300)
  current_iteration <- toString(maxnodes)
  store_maxnode[[current_iteration]] <- rf_maxnode
}
results_mtry <- resamples(store_maxnode)
summary(results_mtry)

## ntrees ##

store_maxtrees <- list()
for (ntree in c(400, 450, 500, 550, 600, 800, 1000, 2000,2500, 2700,3000))
  {
  set.seed(5678)
  rf_maxtrees <- train(Hypothesis,
                       data = District.crimes,
                       method = "rf",
                       metric = "Accuracy",
                       tuneGrid = tuneGrid,
                       trControl = trControl,
                       nodesize = 14,
                       maxnodes = 30,
                       ntree = ntree)
  key <- toString(ntree)
  store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree)

#### Random Forest with tunning  results ############

model.RF <- train(Hypothesis,
                 data = District.crimes,
                 method = "rf",
                 metric = "Accuracy",
                 tuneGrid = tuneGrid,
                 trControl = trControl,
                 nodesize = 14,
                 maxnodes = 30,
                 ntree = 2000)
 
RF_fit <- predict(model.RF, newdata = District.crimes.test)
RF_fit2 <- factor(RF_fit, levels = levels(District.crimes.test$Category))

Accuracy_RF_fit <- confusionMatrix(data = RF_fit2, 
                                         reference = District.crimes.test$
                                         Category)$overall["Accuracy"] 
 
Accuracy_results.SF.Districts <- bind_rows(Accuracy_results.SF.Districts,
                              data_frame(method=
                                "RF tunned on Violent crimes per District",
                                Accuracy = Accuracy_RF_fit))
Accuracy_results.SF.Districts
 
Accuracy_results.SF
```

## **Conclusion** 
The project has required multiple modeling tasks to obtain the best accuracy results. Grouping the data sets was necessary to reduce the area of application and increases the quantity of data per district. Working by districts and the type of crime to predict is the most accurate form of modeling. That is why the models went from 24% to 78% of accuracy.--
The analysis performed shows how to looking for the correct model and tuning the models and recognize what is the best way to approach unknown results. Splitting the project into steps to initiate prediction behavior and using multiple models allows to find better results.--
This analysis could be improved including more data, for instance incorporating Zip Codes of the PdDistrict predictor and the police code for the resolutions. Including those data will allow the SVM model, Euclidean model, to incorporate them and improve accuracy. We can hypothesize more accuracy because PDdistricts and Zipcodes will give specific addresses as gas stations or banks that are more sensitive to violent crime. 

### ** REFERENCES**

Abramson, J.2019. Unit Circle - Sine and Cosine Functions. Retrieved from: https://math.libretexts.org/Bookshelves/Precalculus/Book%3A_Precalculus_(OpenStax)/05%3A_Trigonometric_Functions/5.03%3A_Unit_Circle_-_Sine_and_Cosine_Functions

Irizarry, R.2019. Introduction to Data Science.Retrieved from file:///Users/PedroCornejo/Documents/Data%20Science/datasciencebook.pdf

Barett,M. Why should I use the here package when I'm already using projects?2019. Retrieved from: https://malco.io/2018/11/05/why-should-i-use-the-here-package-when-i-m-already-using-projects/

Developersgoogle,2019. Bucketing. Retrieved from https://developers.google.com/machine-learning/data-prep/transform/bucketing

FBI.2019.NIBRS Offense Codes. retrieved from: https://ucr.fbi.gov/nibrs/2011/resources/nibrs-offense-codes/view

Prabhakaran,S.2017. How to make any plot in ggplot2?. Retrieved from:  http://r-statistics.co/ggplot2-Tutorial-With-R.html

R mutipurose kit.2015. Polar, Spherical and Geographic Coordinates.2015. Retrieved from: https://vvvv.org/blog/polar-spherical-and-geographic-coordinates

R-project-gplots.2019. Package gplots. Retrived from: https://cran.r-project.org/web/packages/gplots/gplots.pdf

Shiyuan and Peng 2019.Preprocessing of categorical predictors in SVM, KNN and KDC. Retrieved from https://stats.libretexts.org/Bookshelves/Advanced_Statistics_Computing/RTG%3A_Classification_Methods/4%3A_Numerical_Experiments_and_Real_Data_Analysis/Preprocessing_of_categorical_predictors_in_SVM%2C_KNN_and_KDC_(contributed_by_Xi_Cheng) 

Trochim,2006.Dummy Variables. Retrieved from https://socialresearchmethods.net/kb/dummyvar.php



 
 
```



